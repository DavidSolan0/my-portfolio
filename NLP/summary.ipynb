{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db933b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399dcbad",
   "metadata": {},
   "source": [
    "### Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d3fdc25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\David.Solano\\Anaconda3\\envs\\David\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "import torch\n",
    "import gensim\n",
    "import networkx as nx\n",
    "import re, unicodedata\n",
    "import dask.dataframe as ddf\n",
    "import gensim.corpora as corpora\n",
    "\n",
    "from dask import delayed\n",
    "from dask import compute\n",
    "from pprint import pprint\n",
    "from itertools import chain\n",
    "from nltk.stem.porter import *\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.manifold import TSNE\n",
    "from transformers import pipeline\n",
    "from prettytable import PrettyTable\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from gensim.models import CoherenceModel\n",
    "from networkx.algorithms import community\n",
    "from rpy2.robjects.packages import importr\n",
    "from gensim.utils import simple_preprocess\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import BertTokenizerFast, EncoderDecoderModel\n",
    "\n",
    "warnings.filterwarnings('ignore', 'SelectableGroups dict interface')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5073147",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('input_path')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6382b457",
   "metadata": {},
   "source": [
    "### Default values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3601d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_name = 'col_name'\n",
    "project_name = 'project_name_'\n",
    "dataset_name = 'file_name.format'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35d8ec1",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f944c604",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(dataset_name,engine ='openpyxl')\n",
    "\n",
    "nRow, nCol = df.shape\n",
    "df = df.set_index('Response ID')\n",
    "print(f'There are {nRow} rows and {nCol} columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcdeb08",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "#### Stopword list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53280ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_add = pd.read_excel('stopword.xlsx',engine='openpyxl')\n",
    "stop_words_add = list(stop_words_add['V1'])\n",
    "stop_words = list(stopwords.words(\"spanish\"))\n",
    "stop_words = stop_words_add + stop_words \n",
    "stop_words = list(dict.fromkeys(stop_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a784764e",
   "metadata": {},
   "source": [
    "#### Preprocessing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13208e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('es_core_news_lg')\n",
    "tknzr = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "\n",
    "def tokenizador(text):\n",
    "    return(' '.join(tknzr.tokenize(text)))\n",
    "\n",
    "def sent_to_words(sentence):\n",
    "    return(gensim.utils.simple_preprocess(str(sentence), deacc=True, min_len = 4))\n",
    "\n",
    "def remove_stopwords(doc):\n",
    "    return ' '.join([word.strip() for word in simple_preprocess(str(doc)) \n",
    "   if word.strip() not in stop_words])\n",
    "\n",
    "def lemmatization(text, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV','INTJ']): \n",
    "    doc = nlp(text)\n",
    "    return  [token.lemma_ for token in doc if token.pos_ in allowed_postags]\n",
    "\n",
    "def clean_text(df, variable = 'ABIERTA CON ORTOGRAFÃA'):\n",
    "    data_lemmatized = df[variable][~df[variable].isna()].map(tokenizador).map(sent_to_words).map(remove_stopwords).map(lemmatization)\n",
    "    \n",
    "    filtro = data_lemmatized.map(len) > 0 \n",
    "    data_lemmatized = data_lemmatized[filtro]\n",
    "    \n",
    "    return data_lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131cd9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "data_lemmatized = clean_text(df,variable = col_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fac0ecf",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3166443e",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(chain.from_iterable(data_lemmatized))\n",
    "pt = PrettyTable(field_names=['Palabra', 'Conteo']) \n",
    "\n",
    "c = Counter(words)\n",
    "[ pt.add_row(kv) for kv in c.most_common()[:10] ]\n",
    "pt.align['Palabra'], pt.align['Conteo'] = 'l', 'r' \n",
    "pt._max_width = {'Palabra':60, 'Conteo':10}\n",
    "print(pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96acb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "str_words = ' '.join(words) \n",
    "wordcloud = WordCloud(width = 800, height = 800,\n",
    "                background_color ='white',\n",
    "                min_font_size = 10).generate(str_words)\n",
    "                    \n",
    "plt.figure(figsize = (10,10), facecolor = None)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad = 0)\n",
    "  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590a3a90",
   "metadata": {},
   "source": [
    "### Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c56c0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [doc for doc in data_lemmatized.map(lambda x: ' '.join(x))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28014c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "ckpt = 'mrm8488/bert2bert_shared-spanish-finetuned-summarization'\n",
    "tokenizer = BertTokenizerFast.from_pretrained(ckpt)\n",
    "model = EncoderDecoderModel.from_pretrained(ckpt).to(device)\n",
    "\n",
    "def generate_summary(text):\n",
    "\n",
    "    inputs = tokenizer([text], padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    input_ids = inputs.input_ids.to(device)\n",
    "    attention_mask = inputs.attention_mask.to(device)\n",
    "    output = model.generate(input_ids, attention_mask=attention_mask)\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "   \n",
    "text = \"Your text here...\"\n",
    "generate_summary(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
